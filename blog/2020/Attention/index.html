<!DOCTYPE html> <html lang="cn"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Attention与Transformer 笔记整理 | Linchang Xiao</title> <meta name="author" content="Linchang Xiao"> <meta name="description" content="Attention与Transformer 笔记整理"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/icon.jpg?7b7b29ac9f6063e03543e5148578c7f5"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://xlcbingo1999.github.io/blog/2020/Attention/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?e74e74bf055e5729d44a7d031a5ca6a5" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?6185d15ea1982787ad7f435576553d64"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Linchang </span>Xiao</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog</a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv</a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Attention与Transformer 笔记整理</h1> <p class="post-meta">April 14, 2020</p> <p class="post-tags"> <a href="/blog/2020"> <i class="fa-solid fa-calendar fa-sm"></i> 2020 </a>   ·   <a href="/blog/tag/%E6%8A%80%E6%9C%AF%E6%9D%82%E8%AE%B0"> <i class="fa-solid fa-hashtag fa-sm"></i> 技术杂记</a>     ·   <a href="/blog/category/%E6%8A%80%E6%9C%AF%E6%9D%82%E8%AE%B0"> <i class="fa-solid fa-tag fa-sm"></i> 技术杂记</a>   </p> </header> <article class="post-content"> <div id="markdown-content"> <h1 id="attention与transformer笔记整理">Attention与Transformer笔记整理</h1> <blockquote> <p>组会20200413 感谢各位大佬的整理。</p> </blockquote> <h2 id="一问题">一、问题</h2> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tranformer(上)：
1. 注意力机制的原理，列举一个应用场景。列举几种注意力的计算方法。
2. 了解残差网络并解释残差连接有什么好处。
3. Layer Normalization的原理
optional: 
4. 对比layer normalization和batch normalization，分析一下二者有什么优劣势，分别适用于什么场景。

transformer(下)：
1. 阐述一下自注意力机制的原理。
2. 描述一下多头注意力机制的原理。
3. transformer如何处理时间顺序。
4. 对比transformer中的encoder和decoder的结构
5. transformer与RNN（LSTM）、CNN的比较（长距离依赖、位置信息、时间复杂度、串行并行）
Optional:
6. 回顾造成神经网络过拟合的原因，并总结一下神经网络防止过拟合的方法。
</code></pre></div></div> <h2 id="二基础内容seq2seq">二、基础内容Seq2seq</h2> <p>transformer已经几乎全面取代RNN了。包括前几天有一篇做SR（speech recognition）的工作也用了transformer，而且SAGAN也是……总之transformer确实是一个非常有效且应用广泛的结构，应该可以算是即seq2seq之后又一次“革命”。</p> <h3 id="21-seq2seq任务">2.1 Seq2seq任务</h3> <p>从一个序列映射到另一个序列的任务，并不关心输入与输出的序列是否长度相等。</p> <p>两种seq2seq</p> <ul> <li>一一对应（词性标注，可以用简单RNN）</li> <li>非一一对应（Encoder-Decoder框架）</li> </ul> <h3 id="22-应用">2.2 应用</h3> <p>机器翻译、词性标注、智能对话。</p> <h3 id="23-encoder-decoder框架">2.3 Encoder-Decoder框架</h3> <p>工作机制：先使用Encoder将输入编码映射到语义空间（通过Encoder网络生成的特征向量），得到一个固定维数的向量，表示<strong>输入语义</strong>。使用Decoder将语义向量解码，获得所需要的输出。如果输出是文本，则Decoder通常就是语言模型。</p> <div class="row mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset=" https://s1.ax1x.com/2020/04/14/GxqNlt-480.webp 480w, https://s1.ax1x.com/2020/04/14/GxqNlt-800.webp 800w, https://s1.ax1x.com/2020/04/14/GxqNlt-1400.webp 1400w, " sizes="95vw" type="image/webp"></source> <img src="https://s1.ax1x.com/2020/04/14/GxqNlt.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="row mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset=" https://s1.ax1x.com/2020/04/14/GxHXgx-480.webp 480w, https://s1.ax1x.com/2020/04/14/GxHXgx-800.webp 800w, https://s1.ax1x.com/2020/04/14/GxHXgx-1400.webp 1400w, " sizes="95vw" type="image/webp"></source> <img src="https://s1.ax1x.com/2020/04/14/GxHXgx.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <p>x作为Encoder的输入，另一个y输入做为Decoder输入，x和y依次按照各自的顺序传入网络。标签y既参与计算loss，也参与节点计算，而不是只做loss监督。C节点是Encoder输出的解码向量，它作为解码Decoder中cell的初始状态，进行对输出的解码。</p> <p>一般其作用为在给定context vector c和所有已预测的词去预测，故t时刻翻译的结果y为以下的联合概率分布。 \(p(y) = \prod_{t=1}^{T}p(y_{t}|(y_{1},...,y_{t-1}),c)\)</p> \[c = h_{T}\] <p>优点：</p> <ul> <li>灵活，不限制Encoder和Decoder使用何种网络，也不限制输入和输出的内容（比如输入可以是图像，输出可以是文本）</li> <li>端到端（end-to-end）的过程，将语义理解和语言生成结合在一起。</li> </ul> <h3 id="24-tensorflow中的seq2seq">2.4 Tensorflow中的seq2seq</h3> <p>本介绍主要使用旧接口。</p> <h4 id="241-实例拟合曲线">2.4.1 实例：拟合曲线</h4> <p>待补充。</p> <h4 id="242-实例股票预测">2.4.2 实例：股票预测</h4> <p>待补充。</p> <h3 id="25-缺点">2.5 缺点</h3> <p>Encoder给出的都是一个固定维度的向量，存在信息损失，如果输入的序列越长，Encoder的输出丢失的原始信息就越多，传入Decoder后，很难在Decoder中有太多特征表现。因此引入有注意力的Seq2seq。</p> <h3 id="26-参考资料">2.6 参考资料</h3> <p>[1] <a href="https://zhuanlan.zhihu.com/p/38485843" rel="external nofollow noopener" target="_blank">https://zhuanlan.zhihu.com/p/38485843</a></p> <p>[2] 《深度学习之TensorFlow 入门、原理与进阶》</p> <h2 id="三基于注意力的seq2seq">三、基于注意力的Seq2Seq</h2> <h3 id="31-attention_seq2seq上1">3.1 attention_seq2seq【上1】</h3> <p>注意力机制，即在生成每个词的时候，对不同的输入词基于不同的关注权重。</p> <p>距离：右侧为输入，上侧为输出。在注意力机制下，对于一个输出网络会自动学习与其对应的输入关系的权重。</p> <div class="row mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset=" https://s1.ax1x.com/2020/04/14/GzZP81-480.webp 480w, https://s1.ax1x.com/2020/04/14/GzZP81-800.webp 800w, https://s1.ax1x.com/2020/04/14/GzZP81-1400.webp 1400w, " sizes="95vw" type="image/webp"></source> <img src="https://s1.ax1x.com/2020/04/14/GzZP81.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <p>比如you (80, 5, 0, 15, 0)，就是模型在生成you的时候的概率分布。对应列表格中值最大的部分对应的输入是“你”。说明模型在输出you的时候最关注的输入词是“你”。</p> <h3 id="32-应用场景上1">3.2 应用场景【上1】</h3> <p>NLP中用于定位关键token或者特征：在一些应用中，比如句子长度特别长的机器翻译场景中，传统的RNN Encoder-Decoder表现非常不理想。一个重要的原因是 t’ 时刻的输出可能更关心输入序列的某些部分是什么内容而和其它部分是什么关系并不大。</p> <p>机器翻译：《深度学习之TensorFlow 入门、原理与进阶》中有代码示例。</p> <p>远距离提取信息：防止RNN在顺序计算中，远距离的信息丢失。</p> <h3 id="32-模型框架">3.2 模型框架</h3> <p>相比于基础Seq2Seq，修改后的模型特点是序列中每个时刻的Encoder生成的c，参与到Decoder的每个序列运算都会经过权重w，那么权重w都以loss的方式通过优化器调节，最终趋向于联系紧密的词。</p> <p>$X = (x_{0},x_{1},x_{2},x_{3})$映射到一个隐层状态 $H = (h_{0},h_{1},h_{2},h_{3})$ 再映射到$Y = (y_{0},y_{1},y_{2})$。Y中的每个元素都和H相连，根据不同的权重对Y赋值。</p> <p>图中红框为attention。</p> <div class="row mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset=" https://s1.ax1x.com/2020/04/14/GxLAnf-480.webp 480w, https://s1.ax1x.com/2020/04/14/GxLAnf-800.webp 800w, https://s1.ax1x.com/2020/04/14/GxLAnf-1400.webp 1400w, " sizes="95vw" type="image/webp"></source> <img src="https://s1.ax1x.com/2020/04/14/GxLAnf.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <h4 id="321-详细解释">3.2.1 详细解释</h4> \[c_{i} = \sum_{j=1}^{T_{x}}a_{ij}h_{ij}\] \[a_{ij} = \frac{exp(e_{ij})}{\sum_{k=1}^{T_{x}}exp(e_{ik})}\] \[e_{ij} = a(s_{j-1}, h_{i})\] <ul> <li>$a_{ij}$的值越高，表示第i个输出在第j个输入上分配的注意力越多，在生成第i个输出的时候受第j个输入的影响越大。</li> <li>$e_{ij}$ encoder i处隐状态和decoder j-1 处的隐状态的匹配 match，此处的 alignment model <em>a</em> 是和其他神经网络一起去训练（即 joint learning），其反映了$h_{j}$的重要性。</li> </ul> <h3 id="33-桶的实现机制">3.3 桶的实现机制</h3> <p>由于输入、输出是可变长的，这给计算带来很大的效率影响。在Tensorflow中使用了一个桶的观念来权衡这个问题，思想就是初始化几个bucket，对数据预处理，按照<strong>每个序列的长短</strong>，将其放到不同的bucket中，小于bucket size部分统一补0来完成对齐的工作，之后就是对不同bucket的批处理计算。</p> <p>这里会进行 <strong>基于权重的交叉熵计算</strong>： 求每个样本loss时对softmax_loss的结果乘以weight，同时乘完后除以weights的总和。</p> <h3 id="34-参考资料">3.4 参考资料</h3> <p>[1] 《深度学习之TensorFlow 入门、原理与进阶》</p> <p>[2] <a href="https://zhuanlan.zhihu.com/p/38485843" rel="external nofollow noopener" target="_blank">https://zhuanlan.zhihu.com/p/38485843</a></p> <p>[3] Bahdanau D, Cho K, Bengio Y. Neural machine translation by jointly learning to align and translate[J]. arXiv preprint arXiv:1409.0473, 2014.</p> <h2 id="四transformer-下4">四、Transformer 【下4】</h2> <h3 id="41-介绍">4.1 介绍</h3> <p>Tranformer是一个升级的Seq2Seq，由一个encoder和一个decoder组成。encoder对输入序列进行编码，encoder将X = (x0, x1, x2,… x{T{x}}) 变成H = (h0, h1, h2,… ,h{T{x}}) ，decoder再将H解码变成Y = (y0, y1, y2,… ,y{T{y}}).</p> <p><strong>encoder和decoder不使用RNN，而是使用多个attention。</strong></p> <h3 id="42-应用">4.2 应用</h3> <p>谷歌团队近期提出的用于生成词向量的BERT算法。</p> <h3 id="43-高层transformer和self-attention">4.3 高层Transformer和self-attention</h3> <ol> <li>在机器翻译中，Transformer可概括为如图</li> </ol> <div class="row mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset=" https://s1.ax1x.com/2020/04/14/GzZn5d-480.webp 480w, https://s1.ax1x.com/2020/04/14/GzZn5d-800.webp 800w, https://s1.ax1x.com/2020/04/14/GzZn5d-1400.webp 1400w, " sizes="95vw" type="image/webp"></source> <img src="https://s1.ax1x.com/2020/04/14/GzZn5d.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <ol> <li>Transformer的本质上是一个Encoder-Decoder的结构</li> </ol> <div class="row mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset=" /assets/img/old_mypic/image-20200407203659415-480.webp 480w, /assets/img/old_mypic/image-20200407203659415-800.webp 800w, /assets/img/old_mypic/image-20200407203659415-1400.webp 1400w, " sizes="95vw" type="image/webp"></source> <img src="/assets/img/old_mypic/image-20200407203659415.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <ol> <li>如论文中所设置的，编码器由6个编码block组成，同样解码器是6个解码block组成。与所有的生成模型相同的是，编码器的输出会作为解码器的输入。【六块可以改成N】</li> </ol> <div class="row mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset=" /assets/img/old_mypic/image-20200407203738688-480.webp 480w, /assets/img/old_mypic/image-20200407203738688-800.webp 800w, /assets/img/old_mypic/image-20200407203738688-1400.webp 1400w, " sizes="95vw" type="image/webp"></source> <img src="/assets/img/old_mypic/image-20200407203738688.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <ol> <li>在Transformer的encoder中，数据首先会经过一个<strong>‘self-attention’</strong>的模块，得到加权后的特征向量Z。</li> </ol> <div class="row mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset=" /assets/img/old_mypic/image-20200407203923903-480.webp 480w, /assets/img/old_mypic/image-20200407203923903-800.webp 800w, /assets/img/old_mypic/image-20200407203923903-1400.webp 1400w, " sizes="95vw" type="image/webp"></source> <img src="/assets/img/old_mypic/image-20200407203923903.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <p>三个不同的向量Query(Q)\Key(K)\Value(V)，长度都是64.他们是通过三个不同的权值矩阵由嵌入向量X(embedding vector) 乘以三个不同的权值矩阵$W_{Q}, W_{K}, W_{V}$ 得到，三个权值矩阵（512x64）。</p> <div class="row mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset=" /assets/img/old_mypic/image-20200407204318798-480.webp 480w, /assets/img/old_mypic/image-20200407204318798-800.webp 800w, /assets/img/old_mypic/image-20200407204318798-1400.webp 1400w, " sizes="95vw" type="image/webp"></source> <img src="/assets/img/old_mypic/image-20200407204318798.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <p>加权后的特征向量Z计算方法：</p> <p>1） 将输入单词转化成嵌入向量；</p> <p>2） 根据嵌入向量得到q,k,v三个向量；</p> <p>3） 为每一个向量计算一个score: score = q*k；</p> <p>4） 为了梯度的稳定，对score归一化，即除以sqrt(d{k}) 【d{k}是什么？下图d{k}等于1】</p> <p>5） 对 score 施加softmax函数；</p> <p>6） softmax点乘Value值v，得到加权的每个输入向量的评分；</p> <p>7） 相加之后得到最终的输出结果 z ： z = SUM(v)。</p> <div class="row mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset=" /assets/img/old_mypic/image-20200407204811811-480.webp 480w, /assets/img/old_mypic/image-20200407204811811-800.webp 800w, /assets/img/old_mypic/image-20200407204811811-1400.webp 1400w, " sizes="95vw" type="image/webp"></source> <img src="/assets/img/old_mypic/image-20200407204811811.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="row mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset=" /assets/img/old_mypic/image-20200407205030601-480.webp 480w, /assets/img/old_mypic/image-20200407205030601-800.webp 800w, /assets/img/old_mypic/image-20200407205030601-1400.webp 1400w, " sizes="95vw" type="image/webp"></source> <img src="/assets/img/old_mypic/image-20200407205030601.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="row mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset=" /assets/img/old_mypic/image-20200407205046138-480.webp 480w, /assets/img/old_mypic/image-20200407205046138-800.webp 800w, /assets/img/old_mypic/image-20200407205046138-1400.webp 1400w, " sizes="95vw" type="image/webp"></source> <img src="/assets/img/old_mypic/image-20200407205046138.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <ol> <li>得到Z之后，会被送到encoder的下一个模块，即Feed Forward Neural Network。这个全连接层有两层，第一层是ReLU，第二层是一个线性激活函数。</li> </ol> <div class="row mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset=" /assets/img/old_mypic/image-20200407205753680-480.webp 480w, /assets/img/old_mypic/image-20200407205753680-800.webp 800w, /assets/img/old_mypic/image-20200407205753680-1400.webp 1400w, " sizes="95vw" type="image/webp"></source> <img src="/assets/img/old_mypic/image-20200407205753680.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="row mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset=" /assets/img/old_mypic/image-20200407205812755-480.webp 480w, /assets/img/old_mypic/image-20200407205812755-800.webp 800w, /assets/img/old_mypic/image-20200407205812755-1400.webp 1400w, " sizes="95vw" type="image/webp"></source> <img src="/assets/img/old_mypic/image-20200407205812755.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <ol> <li>Decoder的结构：多了一个Encoder-Decoder Attention，两个Attention分别用于计算输入和输出的权值。Self-Attention用于翻译和已经翻译的前文之间的关系，Encoder-Decoder Attention用于表示当前翻译和编码的特征向量之间的关系。</li> </ol> <div class="row mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset=" /assets/img/old_mypic/image-20200407210043504-480.webp 480w, /assets/img/old_mypic/image-20200407210043504-800.webp 800w, /assets/img/old_mypic/image-20200407210043504-1400.webp 1400w, " sizes="95vw" type="image/webp"></source> <img src="/assets/img/old_mypic/image-20200407210043504.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <ol> <li> <strong>采用了残差网络中的short-cut结构，目的是解决深度学习中的退化问题</strong>【请参考六】</li> </ol> <div class="row mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset=" /assets/img/old_mypic/image-20200407210559073-480.webp 480w, /assets/img/old_mypic/image-20200407210559073-800.webp 800w, /assets/img/old_mypic/image-20200407210559073-1400.webp 1400w, " sizes="95vw" type="image/webp"></source> <img src="/assets/img/old_mypic/image-20200407210559073.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <h3 id="44-输入编码">4.4 输入编码</h3> <p>输入数据：通过Word2Vec等词嵌入方法将输入语料转化成特征向量，论文中使用的词嵌入的维度为d{model} = 512。</p> <p>在最底层的block中，x将直接作为Transformer的输入，而在其他层中，输入则是上一个block的输出。</p> <div class="row mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset=" /assets/img/old_mypic/image-20200407210316225-480.webp 480w, /assets/img/old_mypic/image-20200407210316225-800.webp 800w, /assets/img/old_mypic/image-20200407210316225-1400.webp 1400w, " sizes="95vw" type="image/webp"></source> <img src="/assets/img/old_mypic/image-20200407210316225.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <h3 id="45-multi-head-attention">4.5 Multi-Head Attention</h3> <p>Multi-Head Attention相当于h个不同的self-attention的集成（ensemble），在这里以8为例子。同self-attention一样，multi-head attention也加入了<strong>short-cut机制</strong>。</p> <ol> <li>将数据X分别输入到8个self-attention中，得到8个加权后的特征矩阵Z{i}；</li> <li>将8个Z{i}按列拼成一个大的特征矩阵；</li> <li>特征矩阵经过一层全连接层后得到Z。</li> </ol> <div class="row mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset=" /assets/img/old_mypic/image-20200407211412600-480.webp 480w, /assets/img/old_mypic/image-20200407211412600-800.webp 800w, /assets/img/old_mypic/image-20200407211412600-1400.webp 1400w, " sizes="95vw" type="image/webp"></source> <img src="/assets/img/old_mypic/image-20200407211412600.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <h3 id="46-encoder-decoder-attention">4.6 Encoder-Decoder Attention</h3> <p>是连接encoder和decoder的一个attention。Q来自于与解码器的上一个输出，K和V则来自于编码器的输出。</p> <p>在机器翻译中，Decode是过程是一个顺序操作的过程，当解码第k个特征向量的时候，只能看到第k-1个及其之前的decode结果。这个时候的multi-head attention是masked multi-head attention。</p> <div class="row mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset=" /assets/img/old_mypic/image-20200407212250291-480.webp 480w, /assets/img/old_mypic/image-20200407212250291-800.webp 800w, /assets/img/old_mypic/image-20200407212250291-1400.webp 1400w, " sizes="95vw" type="image/webp"></source> <img src="/assets/img/old_mypic/image-20200407212250291.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <h3 id="47-损失层">4.7 损失层</h3> <p>Decoder之后，decode的特征向量经过一层激活函数为<strong>softmax</strong>的全连接层之后得到反映每个单词概率的输出向量。此时我们便可以通过CTC等损失函数训练模型了。</p> <h3 id="48-位置编码下3">4.8 位置编码【下3】</h3> <p>Transformer模型并没有捕捉顺序序列的能力，也就是说无论句子的结构怎么打乱，Transformer都会得到类似的结果。换句话说，Transformer只是一个功能更强大的词袋模型而已。</p> <p>在编码词向量时引入了位置编码（Position Embedding）的特征。具体地说，位置编码会在词向量中加入了单词的位置信息，这样Transformer就能区分不同位置的单词了。</p> <p>两种编码模式：</p> <ol> <li>根据数据学习</li> <li>自己设计编码规则【本文使用这个】：位置编码是一个长度为d{model}的特征向量，方便与词向量进行单位加操作。</li> </ol> <div class="row mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset=" /assets/img/old_mypic/image-20200407213000338-480.webp 480w, /assets/img/old_mypic/image-20200407213000338-800.webp 800w, /assets/img/old_mypic/image-20200407213000338-1400.webp 1400w, " sizes="95vw" type="image/webp"></source> <img src="/assets/img/old_mypic/image-20200407213000338.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="row mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset=" /assets/img/old_mypic/image-20200407213101844-480.webp 480w, /assets/img/old_mypic/image-20200407213101844-800.webp 800w, /assets/img/old_mypic/image-20200407213101844-1400.webp 1400w, " sizes="95vw" type="image/webp"></source> <img src="/assets/img/old_mypic/image-20200407213101844.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <p>pos表示单词的位置，i表示单词的维度。</p> <p>这里的意思是将 id 为 pos 的位置映射为一个 d{model} 维的位置向量，这个向量的第 i 个元素的数值就是 PEi(pos)。</p> <p>NLP任务重，除了单词的绝对位置，单词的相对位置也非常重要。根据公式$sin(\alpha+\beta) = sin\alpha·cos\beta + sin\beta·cos\alpha$ 以及$cos(\alpha+\beta) = cos\alpha·cos\beta - sin\beta·sin\alpha$，这表明位置 $k+p$ 的位置向量可以表示为位置 $k$的特征向量的线性变化，这为模型捕捉单词之间的相对位置关系提供了非常大的便利。</p> <h3 id="49-feed-forward">4.9 Feed forward</h3> <p>对于一个输入序列(x0,x1,……x{T})，对每一个x{i}都进行一次channel重组： 512 -&gt; 2048 -&gt; 512 【相当于对整个序列做1*1卷积】</p> <h3 id="410-transformer优点下5">4.10 Transformer优点【下5】</h3> <ul> <li> <p>并行计算，提高训练速度，适合GPU环境</p> <p>Transformer用attention代替了原本的RNN; 而RNN在训练的时候, 当前step的计算要依赖于上一个step的hidden state的, 也就是说这是一个sequential procedure, 我每次计算都要等之前的计算完成才能展开. 而Transformer不用RNN, 所有的计算都可以并行进行, 从而提高的训练的速度.</p> </li> <li> <p>建立直接的长距离连接</p> <p>将任意两个单词的距离是1。原本的RNN里, 如果第一帧要和第十帧建立依赖, 那么第一帧的数据要依次经过第二三四五…九帧传给第十帧, 进而产生二者的计算. 而在这个传递的过程中, 可能第一帧的数据已经产生了<strong>biased</strong>, 因此这个交互的速度和准确性都没有保障. 而在Transformer中, 由于有<strong>self attention</strong>的存在, <strong>任意两帧之间都有直接的交互, 从而建立了直接的依赖</strong>, 无论二者距离多远。</p> </li> <li> <p>超越NLP机器翻译领域</p> </li> </ul> <h3 id="411-transformer缺点">4.11 Transformer缺点</h3> <ul> <li>使模型丧失了捕捉局部特征的能力，RNN + CNN + Transformer的结合可能会带来更好的效果。</li> <li>Transformer失去的位置信息其实在NLP中非常重要，而论文中在特征向量中加入Position Embedding也只是一个权宜之计，并没有改变Transformer结构上的固有缺陷。</li> </ul> <h3 id="412-参考资料">4.12 参考资料</h3> <p>[1] <a href="https://zhuanlan.zhihu.com/p/48508221" rel="external nofollow noopener" target="_blank">https://zhuanlan.zhihu.com/p/48508221</a></p> <p>[2] <a href="https://zhuanlan.zhihu.com/p/38485843" rel="external nofollow noopener" target="_blank">https://zhuanlan.zhihu.com/p/38485843</a></p> <p>[3] <a href="https://www.infoq.cn/article/lteUOi30R4uEyy740Ht2" rel="external nofollow noopener" target="_blank">https://www.infoq.cn/article/lteUOi30R4uEyy740Ht2</a></p> <h2 id="五残差网络">五、残差网络</h2> <h3 id="51-背景">5.1 背景</h3> <p>在深度学习中，网络层数增多一般会伴着下面几个问题：</p> <ol> <li>计算资源的消耗【通过GPU集群解决】</li> <li>模型容易过拟合【通过采样海量数据，配合Dropout正则化】</li> <li>梯度消失/梯度爆炸问题的产生【Batch Normalization】</li> <li>网络退化现象（degradation）：训练集loss逐渐下降，然后趋于饱和，当你再增加网络深度的话，训练集loss反而会增大</li> </ol> <h3 id="52-残差网络">5.2 残差网络</h3> <p>残差网络是由一系列残差块组成的。</p> <div class="row mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset=" /assets/img/old_mypic/image-20200407224113733-480.webp 480w, /assets/img/old_mypic/image-20200407224113733-800.webp 800w, /assets/img/old_mypic/image-20200407224113733-1400.webp 1400w, " sizes="95vw" type="image/webp"></source> <img src="/assets/img/old_mypic/image-20200407224113733.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <p>残差块分成两部分直接映射部分和残差部分。 $h_{x_{l}}$ 是直接映射，反应在图1中是左边的曲线； $F(x_{l},W_{l})$ 是残差部分，一般由两个或者三个卷积操作构成，即图1中右侧包含卷积的部分。Weight在卷积网络中是指卷积操作，addition是指单位加操作。</p> <p>卷积网络中，x{l}和x{l+1}的feature map的数量不一样，需要使用1x1卷积进行升维和降维。$h(x_{l}) = W’_{l}x$</p> <div class="row mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset=" /assets/img/old_mypic/image-20200407224351074-480.webp 480w, /assets/img/old_mypic/image-20200407224351074-800.webp 800w, /assets/img/old_mypic/image-20200407224351074-1400.webp 1400w, " sizes="95vw" type="image/webp"></source> <img src="/assets/img/old_mypic/image-20200407224351074.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">res_block_v1</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">input_filter</span><span class="p">,</span> <span class="n">output_filter</span><span class="p">):</span>
    <span class="n">res_x</span> <span class="o">=</span> <span class="nc">Conv2D</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">),</span> <span class="n">filters</span><span class="o">=</span><span class="n">output_filter</span><span class="p">,</span> <span class="n">strides</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="sh">'</span><span class="s">same</span><span class="sh">'</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">res_x</span> <span class="o">=</span> <span class="nc">BatchNormalization</span><span class="p">()(</span><span class="n">res_x</span><span class="p">)</span>
    <span class="n">res_x</span> <span class="o">=</span> <span class="nc">Activation</span><span class="p">(</span><span class="sh">'</span><span class="s">relu</span><span class="sh">'</span><span class="p">)(</span><span class="n">res_x</span><span class="p">)</span>
    <span class="n">res_x</span> <span class="o">=</span> <span class="nc">Conv2D</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">),</span> <span class="n">filters</span><span class="o">=</span><span class="n">output_filter</span><span class="p">,</span> <span class="n">strides</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="sh">'</span><span class="s">same</span><span class="sh">'</span><span class="p">)(</span><span class="n">res_x</span><span class="p">)</span>
    <span class="n">res_x</span> <span class="o">=</span> <span class="nc">BatchNormalization</span><span class="p">()(</span><span class="n">res_x</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">input_filter</span> <span class="o">==</span> <span class="n">output_filter</span><span class="p">:</span>
        <span class="n">identity</span> <span class="o">=</span> <span class="n">x</span>
    <span class="k">else</span><span class="p">:</span> <span class="c1">#需要升维或者降维
</span>        <span class="n">identity</span> <span class="o">=</span> <span class="nc">Conv2D</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span> <span class="n">filters</span><span class="o">=</span><span class="n">output_filter</span><span class="p">,</span> <span class="n">strides</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="sh">'</span><span class="s">same</span><span class="sh">'</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nf">add</span><span class="p">([</span><span class="n">identity</span><span class="p">,</span> <span class="n">res_x</span><span class="p">])</span>
    <span class="n">output</span> <span class="o">=</span> <span class="nc">Activation</span><span class="p">(</span><span class="sh">'</span><span class="s">relu</span><span class="sh">'</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">output</span>
</code></pre></div></div> <h3 id="53-残差网络的搭建">5.3 残差网络的搭建</h3> <ol> <li>使用VGG公式搭建Plain VGG网络</li> <li>在Plain VGG的卷积网络之间插入Identity Mapping，注意需要升维或降维的时候加入1X1卷积。</li> </ol> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">resnet_v1</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="nc">Conv2D</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">),</span> <span class="n">filters</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">strides</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="sh">'</span><span class="s">same</span><span class="sh">'</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="sh">'</span><span class="s">relu</span><span class="sh">'</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="nf">res_block_v1</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="nf">res_block_v1</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">32</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="nc">Flatten</span><span class="p">()(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="nc">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="sh">'</span><span class="s">softmax</span><span class="sh">'</span><span class="p">,</span> <span class="n">kernel_initializer</span><span class="o">=</span><span class="sh">'</span><span class="s">he_normal</span><span class="sh">'</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">outputs</span>
</code></pre></div></div> <h3 id="54-残差与误差">5.4 残差与误差</h3> <p>【y=x是观测值、 H(x) 是预测值】</p> <h5 id="水位线模型预测10m你测量的是104m观测值但真实值为105m真实性未知"><strong>水位线：模型预测10m，你测量的是10.4m【观测值】，但真实值为10.5m？真实性未知！</strong></h5> <p>衡量一个残差块：H(x) = F(x) + x</p> <ul> <li>误差：衡量观测值和真实值之间的差距</li> <li>残差：预测值和观测值之间的差距 F(x) = H(x) - x</li> </ul> <h3 id="55-残差网络的原理">5.5 残差网络的原理</h3> <div class="row mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset=" /assets/img/old_mypic/image-20200407225339983-480.webp 480w, /assets/img/old_mypic/image-20200407225339983-800.webp 800w, /assets/img/old_mypic/image-20200407225339983-1400.webp 1400w, " sizes="95vw" type="image/webp"></source> <img src="/assets/img/old_mypic/image-20200407225339983.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <p>不考虑升维和降维的情况，h(·)是直接映射，f(·)是激活函数，一般是ReLU。</p> <ul> <li>假设1：h(·)是直接映射</li> <li>假设2：f(·)是直接映射</li> </ul> <p>残差块：</p> <div class="row mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset=" /assets/img/old_mypic/image-20200407225703608-480.webp 480w, /assets/img/old_mypic/image-20200407225703608-800.webp 800w, /assets/img/old_mypic/image-20200407225703608-1400.webp 1400w, " sizes="95vw" type="image/webp"></source> <img src="/assets/img/old_mypic/image-20200407225703608.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <p>对于一个更深层L，其与l层的关系表示为：</p> <div class="row mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset=" /assets/img/old_mypic/image-20200407225735065-480.webp 480w, /assets/img/old_mypic/image-20200407225735065-800.webp 800w, /assets/img/old_mypic/image-20200407225735065-1400.webp 1400w, " sizes="95vw" type="image/webp"></source> <img src="/assets/img/old_mypic/image-20200407225735065.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <p>公式反应的残差网络的两个属性：</p> <ol> <li>L层可以表示为任何一个比它浅的l层和他们之间的残差部分之和；</li> <li>L是各个残差块特征的单位累核，而MLP是特征矩阵的累积。【多层感知机（MLP，Multilayer Perceptron）也叫人工神经网络（ANN，Artificial Neural Network），除了输入输出层，它中间可以有多个隐层，最简单的MLP只含一个隐层】</li> </ol> <div class="row mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset=" /assets/img/old_mypic/image-20200407230121460-480.webp 480w, /assets/img/old_mypic/image-20200407230121460-800.webp 800w, /assets/img/old_mypic/image-20200407230121460-1400.webp 1400w, " sizes="95vw" type="image/webp"></source> <img src="/assets/img/old_mypic/image-20200407230121460.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <p>残差网络的梯度：</p> <div class="row mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset=" /assets/img/old_mypic/image-20200407230357773-480.webp 480w, /assets/img/old_mypic/image-20200407230357773-800.webp 800w, /assets/img/old_mypic/image-20200407230357773-1400.webp 1400w, " sizes="95vw" type="image/webp"></source> <img src="/assets/img/old_mypic/image-20200407230357773.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <p>公式反应的残差网络的两个属性：</p> <ol> <li>在整个训练过程中，</li> </ol> <div class="row mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset=" /assets/img/old_mypic/image-20200407230430137-480.webp 480w, /assets/img/old_mypic/image-20200407230430137-800.webp 800w, /assets/img/old_mypic/image-20200407230430137-1400.webp 1400w, " sizes="95vw" type="image/webp"></source> <img src="/assets/img/old_mypic/image-20200407230430137.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <p>不可能一直为-1，也就是残差网络中不会出现梯度消失的问题。</p> <ol> <li>$\frac{\partial \epsilon}{\partial x_{L}}$表示L层的梯度可以直接传递到任何一个比它浅的l层。</li> </ol> <p>当残差块满足上面<strong>两个假设</strong>时，信息可以非常畅通的在高层和低层之间相互传导，说明这两个假设是让残差网络可以训练深度模型的充分条件。</p> <h4 id="551-直接映射是最好的残差网络的选择">5.5.1 直接映射是最好的残差网络的选择</h4> <p>对于假设1，采用反证法，假设</p> <div class="row mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset=" /assets/img/old_mypic/image-20200407231715828-480.webp 480w, /assets/img/old_mypic/image-20200407231715828-800.webp 800w, /assets/img/old_mypic/image-20200407231715828-1400.webp 1400w, " sizes="95vw" type="image/webp"></source> <img src="/assets/img/old_mypic/image-20200407231715828.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <p>，残差块表示为：</p> <div class="row mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset=" D:\Work and study\Study\Nlp\组会20200413.assets\image-20200407231741281-480.webp 480w, D:\Work and study\Study\Nlp\组会20200413.assets\image-20200407231741281-800.webp 800w, D:\Work and study\Study\Nlp\组会20200413.assets\image-20200407231741281-1400.webp 1400w, " sizes="95vw" type="image/webp"></source> <img src="D:\Work%20and%20study\Study\Nlp\%E7%BB%84%E4%BC%9A20200413.assets\image-20200407231741281.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <p>](assets/img/old_mypic/image-20200407231741281.png)</p> <p>对于更深层次的L层 \(x_{L} = (\prod_{i=l}^{L-1}\lambda_{l})x_{l} + \sum_{i=l}^{L-1}(\prod_{i=l}^{L-1} F(x_{l},W_{l}))\) 如果只考虑左边的情况，损失函数对x{l}求偏微分得到： \(\frac{\partial \epsilon}{\partial x_{l}} = (\prod_{i=l}^{L-1}\lambda_{l})\) 公式反应的属性：</p> <ol> <li>$\lambda$&gt;1 ，可能会发生梯度爆炸。</li> <li>$\lambda$&lt;1 ， 梯度变成0，会阻碍残差网络信息的方向传递，从而影响残差网络的训练。</li> <li>$\lambda$=1 是必要条件。</li> </ol> <p>不同网络的差距：</p> <div class="row mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset=" D:\Work and study\Study\Nlp\组会20200413.assets\image-20200407233933533-480.webp 480w, D:\Work and study\Study\Nlp\组会20200413.assets\image-20200407233933533-800.webp 800w, D:\Work and study\Study\Nlp\组会20200413.assets\image-20200407233933533-1400.webp 1400w, " sizes="95vw" type="image/webp"></source> <img src="D:\Work%20and%20study\Study\Nlp\%E7%BB%84%E4%BC%9A20200413.assets\image-20200407233933533.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <p>](assets/img/old_mypic/image-20200407233933533.png)</p> <div class="row mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset=" https://pic4.zhimg.com/v2-5d8fd2868a4ba30e61ce477ab00d7f0f_r-480.webp 480w, https://pic4.zhimg.com/v2-5d8fd2868a4ba30e61ce477ab00d7f0f_r-800.webp 800w, https://pic4.zhimg.com/v2-5d8fd2868a4ba30e61ce477ab00d7f0f_r-1400.webp 1400w, " sizes="95vw" type="image/webp"></source> <img src="https://pic4.zhimg.com/v2-5d8fd2868a4ba30e61ce477ab00d7f0f_r.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <p>所以假设一成立，即：</p> <div class="row mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset=" D:\Work and study\Study\Nlp\组会20200413.assets\image-20200407234708691-480.webp 480w, D:\Work and study\Study\Nlp\组会20200413.assets\image-20200407234708691-800.webp 800w, D:\Work and study\Study\Nlp\组会20200413.assets\image-20200407234708691-1400.webp 1400w, " sizes="95vw" type="image/webp"></source> <img src="D:\Work%20and%20study\Study\Nlp\%E7%BB%84%E4%BC%9A20200413.assets\image-20200407234708691.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <p>](assets/img/old_mypic/image-20200407234708691.png)</p> <h4 id="552-激活函数的位置">5.5.2 激活函数的位置</h4> <p>BN：batch norm</p> <div class="row mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset=" D:\Work and study\Study\Nlp\组会20200413.assets\image-20200407234946413-480.webp 480w, D:\Work and study\Study\Nlp\组会20200413.assets\image-20200407234946413-800.webp 800w, D:\Work and study\Study\Nlp\组会20200413.assets\image-20200407234946413-1400.webp 1400w, " sizes="95vw" type="image/webp"></source> <img src="D:\Work%20and%20study\Study\Nlp\%E7%BB%84%E4%BC%9A20200413.assets\image-20200407234946413.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <p>](assets/img/old_mypic/image-20200407234946413.png)</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">res_block_v2</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">input_filter</span><span class="p">,</span> <span class="n">output_filter</span><span class="p">):</span>
    <span class="n">res_x</span> <span class="o">=</span> <span class="nc">BatchNormalization</span><span class="p">()(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">res_x</span> <span class="o">=</span> <span class="nc">Activation</span><span class="p">(</span><span class="sh">'</span><span class="s">relu</span><span class="sh">'</span><span class="p">)(</span><span class="n">res_x</span><span class="p">)</span>
    <span class="n">res_x</span> <span class="o">=</span> <span class="nc">Conv2D</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">),</span> <span class="n">filters</span><span class="o">=</span><span class="n">output_filter</span><span class="p">,</span> <span class="n">strides</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="sh">'</span><span class="s">same</span><span class="sh">'</span><span class="p">)(</span><span class="n">res_x</span><span class="p">)</span>
    <span class="n">res_x</span> <span class="o">=</span> <span class="nc">BatchNormalization</span><span class="p">()(</span><span class="n">res_x</span><span class="p">)</span>
    <span class="n">res_x</span> <span class="o">=</span> <span class="nc">Activation</span><span class="p">(</span><span class="sh">'</span><span class="s">relu</span><span class="sh">'</span><span class="p">)(</span><span class="n">res_x</span><span class="p">)</span>
    <span class="n">res_x</span> <span class="o">=</span> <span class="nc">Conv2D</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">),</span> <span class="n">filters</span><span class="o">=</span><span class="n">output_filter</span><span class="p">,</span> <span class="n">strides</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="sh">'</span><span class="s">same</span><span class="sh">'</span><span class="p">)(</span><span class="n">res_x</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">input_filter</span> <span class="o">==</span> <span class="n">output_filter</span><span class="p">:</span>
        <span class="n">identity</span> <span class="o">=</span> <span class="n">x</span>
    <span class="k">else</span><span class="p">:</span> <span class="c1">#需要升维或者降维
</span>        <span class="n">identity</span> <span class="o">=</span> <span class="nc">Conv2D</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span> <span class="n">filters</span><span class="o">=</span><span class="n">output_filter</span><span class="p">,</span> <span class="n">strides</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="sh">'</span><span class="s">same</span><span class="sh">'</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">output</span><span class="o">=</span> <span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nf">add</span><span class="p">([</span><span class="n">identity</span><span class="p">,</span> <span class="n">res_x</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">output</span>

<span class="k">def</span> <span class="nf">resnet_v2</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="nc">Conv2D</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">),</span> <span class="n">filters</span><span class="o">=</span><span class="mi">16</span> <span class="p">,</span> <span class="n">strides</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="sh">'</span><span class="s">same</span><span class="sh">'</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="sh">'</span><span class="s">relu</span><span class="sh">'</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="nf">res_block_v2</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="nf">res_block_v2</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">32</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="nc">BatchNormalization</span><span class="p">()(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="nc">Flatten</span><span class="p">()(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="nc">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="sh">'</span><span class="s">softmax</span><span class="sh">'</span><span class="p">,</span> <span class="n">kernel_initializer</span><span class="o">=</span><span class="sh">'</span><span class="s">he_normal</span><span class="sh">'</span><span class="p">)(</span><span class="n">y</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">outputs</span>
</code></pre></div></div> <h4 id="553-残差网络与模型集成">5.5.3 残差网络与模型集成</h4> <p>对于一个3层的残差网络可以展开成一棵含有8个节点的二叉树，而最终的输出便是这8个节点的集成。而他们的实验也验证了这一点，随机删除残差网络的一些节点网络的性能变化较为平滑，而对于VGG等stack到一起的网络来说，随机删除一些节点后，网络的输出将完全随机。</p> <div class="row mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset=" D:\Work and study\Study\Nlp\组会20200413.assets\image-20200407235341710-480.webp 480w, D:\Work and study\Study\Nlp\组会20200413.assets\image-20200407235341710-800.webp 800w, D:\Work and study\Study\Nlp\组会20200413.assets\image-20200407235341710-1400.webp 1400w, " sizes="95vw" type="image/webp"></source> <img src="D:\Work%20and%20study\Study\Nlp\%E7%BB%84%E4%BC%9A20200413.assets\image-20200407235341710.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <p>](assets/img/old_mypic/image-20200407235341710.png)</p> <h3 id="56-参考资料">5.6 参考资料</h3> <h2 id="六normalization-归一化上3上4">六、Normalization 归一化【上3/上4】</h2> <h3 id="61-使用normalization的原因">6.1 使用Normalization的原因</h3> <h4 id="611-独立同分布与白化">6.1.1 独立同分布与白化</h4> <p>独立同分布的数据可以简化常规机器学习模型的训练、提升机器学习模型的预测能力，已经是一个共识。</p> <p>把数据喂给机器学习模型之前，“<strong>白化（whitening）</strong>”是一个重要的数据预处理步骤。其具有两个目的：</p> <ol> <li>去除特征间的相关性 -&gt; 独立</li> <li>使得所有的特征巨有相同的均值和方差 -&gt; 同分布</li> </ol> <p>主要的方法：PCA</p> <h4 id="612--internal-covariate-shift-内部协方差移位">6.1.2 Internal Covariate Shift (内部协方差移位)</h4> <p>深度神经网络涉及到很多层的叠加，而每一层的参数更新会导致上层的输入数据分布发生变化，通过层层叠加，高层的输入分布变化会非常剧烈，这就使得高层需要不断去重新适应底层的参数更新。为了训好模型，我们需要非常谨慎地去设定学习率、初始化权重、以及尽可能细致的参数更新策略。</p> <p>于神经网络的各层输出，由于它们经过了层内操作作用，其分布显然与各层对应的输入信号分布不同，而且差异会随着网络深度增大而增大，可是它们所能“指示”的样本标记（label）仍然是不变的，这便符合了covariate shift的定义。</p> <h4 id="613-ics的问题">6.1.3 ICS的问题</h4> <p>每个神经元的输入数据不再是“独立同分布”。</p> <p>其一，上层参数需要不断适应新的输入数据分布，降低学习速度。</p> <p>其二，下层输入的变化可能趋向于变大或者变小，导致上层落入饱和区，使得学习过早停止。</p> <p>其三，每层的更新都会影响到其它层，因此每层的参数更新策略需要尽可能的谨慎。</p> <h3 id="62-normalization的通用框架和基本思想">6.2 Normalization的通用框架和基本思想</h3> <p>神经元接收一组输入向量$X=(x_{1},x_{2},…,x_{d})$，通过某种运算得到一个标量值$y = f(x)$</p> <p>由于ICS的存在，x的分布会相差很大，所以要进行白化。但是白化太昂贵，于是就退而求其次，使用Normalization。在发送给神经元的时候，对x进行<strong>平移和伸缩变化</strong>。</p> <div class="row mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset=" D:\Work and study\Study\Nlp\组会20200413.assets\image-20200408001110390-480.webp 480w, D:\Work and study\Study\Nlp\组会20200413.assets\image-20200408001110390-800.webp 800w, D:\Work and study\Study\Nlp\组会20200413.assets\image-20200408001110390-1400.webp 1400w, " sizes="95vw" type="image/webp"></source> <img src="D:\Work%20and%20study\Study\Nlp\%E7%BB%84%E4%BC%9A20200413.assets\image-20200408001110390.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <p>](assets/img/old_mypic/image-20200408001110390.png) \(\mu 是平移参数， \sigma 是缩放参数，通过这两个参数进行shift和scale变换:\hat x = \frac{x-\mu}{\sigma}\) 操作得到均值为0，方差为1的标准分布。 \(b是再平移参数，g是再缩放参数，y = g·\hat x + b\)</p> <p>\(b是再平移参数，g是再缩放参数，y = g·\hat x + b\) 操作得到均值为b，方差为g^2的标准分布。【<strong>为了保证模型的表达能力不因为规范化而下降</strong>。】通过g和b参数把激活输入值从标准正态分布左移或者右移一点并长胖一点或者变瘦一点，每个实例挪动的程度不一样，这样等价于把非线性函数的值从正中心周围的线性区往非线性区动了动。<strong>核心思想应该是想找到一个线性和非线性的较好平衡点，既能享受非线性的较强表达能力的好处，又避免太靠非线性区两头使得网络收敛速度太慢。</strong></p> <p>原因：</p> <ol> <li> <p>第一步的变换将输入数据限制到了一个全局统一的确定范围（均值为 0、方差为 1）。下层神经元可能很努力地在学习，但不论其如何变化，其输出的结果在交给上层神经元进行处理之前，将被粗暴地重新调整到这一固定范围。</p> </li> <li> <p>规范化后的数据进行再平移和再缩放，使得每个神经元对应的输入范围是针对该神经元量身定制的一个确定范围。rescale 和 reshift 的参数都是可学习的，这就使得 Normalization 层可以学习如何去尊重底层的学习结果。</p> </li> <li> <p>保证获得非线性的表达能力。Sigmoid 等激活函数在神经网络中有着重要作用，通过区分饱和区和非饱和区，使得神经网络的数据变换具有了非线性计算能力。而第一步的规范化会将几乎所有数据映射到激活函数的非饱和区（线性区），仅利用到了线性变化能力，从而降低了神经网络的表达能力。而进行再变换，则可以将数据从线性区变换到非线性区，恢复模型的表达能力。</p> </li> </ol> <div class="row mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset=" D:\Work and study\Study\Nlp\组会20200413.assets\image-20200408110231170-480.webp 480w, D:\Work and study\Study\Nlp\组会20200413.assets\image-20200408110231170-800.webp 800w, D:\Work and study\Study\Nlp\组会20200413.assets\image-20200408110231170-1400.webp 1400w, " sizes="95vw" type="image/webp"></source> <img src="D:\Work%20and%20study\Study\Nlp\%E7%BB%84%E4%BC%9A20200413.assets\image-20200408110231170.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <p>](assets/img/old_mypic/image-20200408110231170.png)</p> <h3 id="63-主流normalization方法梳理">6.3 主流Normalization方法梳理</h3> <div class="row mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset=" D:\Work and study\Study\Nlp\组会20200413.assets\image-20200408112942948-480.webp 480w, D:\Work and study\Study\Nlp\组会20200413.assets\image-20200408112942948-800.webp 800w, D:\Work and study\Study\Nlp\组会20200413.assets\image-20200408112942948-1400.webp 1400w, " sizes="95vw" type="image/webp"></source> <img src="D:\Work%20and%20study\Study\Nlp\%E7%BB%84%E4%BC%9A20200413.assets\image-20200408112942948.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <p>](assets/img/old_mypic/image-20200408112942948.png)</p> <h4 id="631-batch-normalization---纵向规范化">6.3.1 Batch Normalization - 纵向规范化</h4> <div class="row mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset=" D:\Work and study\Study\Nlp\组会20200413.assets\image-20200408113001609-480.webp 480w, D:\Work and study\Study\Nlp\组会20200413.assets\image-20200408113001609-800.webp 800w, D:\Work and study\Study\Nlp\组会20200413.assets\image-20200408113001609-1400.webp 1400w, " sizes="95vw" type="image/webp"></source> <img src="D:\Work%20and%20study\Study\Nlp\%E7%BB%84%E4%BC%9A20200413.assets\image-20200408113001609.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <p>](assets/img/old_mypic/image-20200408113001609.png)</p> <div class="row mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset=" D:\Work and study\Study\Nlp\组会20200413.assets\image-20200408002014058-480.webp 480w, D:\Work and study\Study\Nlp\组会20200413.assets\image-20200408002014058-800.webp 800w, D:\Work and study\Study\Nlp\组会20200413.assets\image-20200408002014058-1400.webp 1400w, " sizes="95vw" type="image/webp"></source> <img src="D:\Work%20and%20study\Study\Nlp\%E7%BB%84%E4%BC%9A20200413.assets\image-20200408002014058.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <p>](assets/img/old_mypic/image-20200408002014058.png)</p> <ol> <li>利用网络训练时一个 mini-batch 的数据来计算该神经元 $x_{i}$ 的均值和方差,因而称为 Batch Normalization。</li> </ol> <div class="row mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset=" D:\Work and study\Study\Nlp\组会20200413.assets\image-20200408111021890-480.webp 480w, D:\Work and study\Study\Nlp\组会20200413.assets\image-20200408111021890-800.webp 800w, D:\Work and study\Study\Nlp\组会20200413.assets\image-20200408111021890-1400.webp 1400w, " sizes="95vw" type="image/webp"></source> <img src="D:\Work%20and%20study\Study\Nlp\%E7%BB%84%E4%BC%9A20200413.assets\image-20200408111021890.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <p>](assets/img/old_mypic/image-20200408111021890.png)</p> <p>m是 mini-batch的大小。</p> <p>按上图所示，相对于一层神经元的水平排列，BN 可以看做一种纵向的规范化。由于 BN 是针对单个维度定义的，因此标准公式中的计算均为 element-wise 的。</p> <p>BN 独立地规范化每一个输入维度x{i} ，但规范化的参数是一个 mini-batch 的一阶统计量和二阶统计量。这就要求 每一个 mini-batch 的统计量是整体统计量的近似估计，或者说每一个 mini-batch 彼此之间，以及和整体数据，都应该是近似同分布的。分布差距较小的 mini-batch 可以看做是为规范化操作和模型训练引入了噪声，可以增加模型的鲁棒性；但如果每个 mini-batch的原始分布差别很大，那么不同 mini-batch 的数据将会进行不一样的数据变换，这就增加了模型训练的难度。</p> <ol> <li> <p>BN 比较适用的场景是：每个 mini-batch 比较大，数据分布比较接近。在进行训练之前，要做好充分的 shuffle. 否则效果会差很多。，由于 BN 需要在运行过程中统计每个 mini-batch 的一阶统计量和二阶统计量，因此<strong>不适用于 动态的网络结构 和 RNN 网络。</strong></p> </li> <li> <p><strong>激活输入值</strong>（就是深度神经网络中每个隐层在<strong>进行非线性变换处理前的数据</strong>，即<strong>BN一定是用在激活函数之前的！！</strong>）</p> </li> </ol> <div class="row mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset=" D:\Work and study\Study\Nlp\组会20200413.assets\image-20200408110249089-480.webp 480w, D:\Work and study\Study\Nlp\组会20200413.assets\image-20200408110249089-800.webp 800w, D:\Work and study\Study\Nlp\组会20200413.assets\image-20200408110249089-1400.webp 1400w, " sizes="95vw" type="image/webp"></source> <img src="D:\Work%20and%20study\Study\Nlp\%E7%BB%84%E4%BC%9A20200413.assets\image-20200408110249089.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <p>](assets/img/old_mypic/image-20200408110249089.png)</p> <p>训练收敛慢的原因：一般是整体分布逐渐往非线性函数的取值区间的上下限两端靠近（对于Sigmoid函数来说，在极大或极小的值处函数的导数接近0），所以这导致反向传播时低层神经网络的梯度消失，这是训练深层神经网络收敛越来越慢的<strong>本质原因</strong>。</p> <p>BN就是通过一定的规范化手段，把每层神经网络任意神经元这个激活输入值的分布强行拉回到均值为0方差为1的标准正态分布，这样使得激活输入值落在非线性函数对<strong>输入比较敏感的区域</strong>（例如sigmoid函数的中间的部分），这样输入的小变化就会导致损失函数较大的变化，意思是这样<strong>让梯度变大</strong>，避免梯度消失问题产生，而且梯度变大意味着学习收敛速度快，能大大加快训练速度。<strong>经过BN后，目前大部分激活输入值都落入非线性函数的线性区内（近似线性区域），其对应的导数远离导数饱和区，这样来加速训练收敛过程。</strong></p> <ol> <li>优点：</li> </ol> <ul> <li>提高训练速度、收敛过程大大加快；</li> <li>增加分类效果，防止过拟合；</li> <li>方便调参，可以使用大的学习率</li> </ul> <ol> <li>局限：</li> </ol> <ul> <li>BN适用于batch size较大且各mini-batch分布相近似的场景下（训练前需进行充分的shuffle）。不适用于动态网络结构和RNN。其次，BN只在训练的时候用，inference的时候不会用到，因为inference的输入不是批量输入。</li> </ul> <h4 id="632-layer-normalization---横向规范化">6.3.2 Layer Normalization - 横向规范化</h4> <div class="row mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset=" D:\Work and study\Study\Nlp\组会20200413.assets\image-20200408113019895-480.webp 480w, D:\Work and study\Study\Nlp\组会20200413.assets\image-20200408113019895-800.webp 800w, D:\Work and study\Study\Nlp\组会20200413.assets\image-20200408113019895-1400.webp 1400w, " sizes="95vw" type="image/webp"></source> <img src="D:\Work%20and%20study\Study\Nlp\%E7%BB%84%E4%BC%9A20200413.assets\image-20200408113019895.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <p>](assets/img/old_mypic/image-20200408113019895.png)</p> <div class="row mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset=" D:\Work and study\Study\Nlp\组会20200413.assets\image-20200408002349398-480.webp 480w, D:\Work and study\Study\Nlp\组会20200413.assets\image-20200408002349398-800.webp 800w, D:\Work and study\Study\Nlp\组会20200413.assets\image-20200408002349398-1400.webp 1400w, " sizes="95vw" type="image/webp"></source> <img src="D:\Work%20and%20study\Study\Nlp\%E7%BB%84%E4%BC%9A20200413.assets\image-20200408002349398.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <p>](assets/img/old_mypic/image-20200408002349398.png)</p> <p>层规范化就是针对 BN 的上述不足而提出的。与 BN 不同，LN 是一种横向的规范化，如图所示。它综合考虑一层所有维度的输入，计算该层的平均输入值和输入方差，然后用同一个规范化操作来转换各个维度的输入。</p> <div class="row mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset=" D:\Work and study\Study\Nlp\组会20200413.assets\image-20200408112015761-480.webp 480w, D:\Work and study\Study\Nlp\组会20200413.assets\image-20200408112015761-800.webp 800w, D:\Work and study\Study\Nlp\组会20200413.assets\image-20200408112015761-1400.webp 1400w, " sizes="95vw" type="image/webp"></source> <img src="D:\Work%20and%20study\Study\Nlp\%E7%BB%84%E4%BC%9A20200413.assets\image-20200408112015761.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <p>](assets/img/old_mypic/image-20200408112015761.png)</p> <p>H是某个隐藏层的神经元数量。</p> <p>LN 针对单个训练样本进行，不依赖于其他数据，因此可以避免 BN 中受 mini-batch 数据分布影响的问题，可以用于 小mini-batch场景、动态网络场景和 RNN，特别是自然语言处理领域。此外，LN 不需要保存 mini-batch 的均值和方差，节省了额外的存储空间。</p> <p>LN 对于一整层的神经元训练得到同一个转换——所有的输入都在同一个区间范围内。如果不同输入特征不属于相似的类别（比如颜色和大小），那么 LN 的处理可能会降低模型的表达能力。</p> <p>优点：不需要批训练，在单条数据内部就能归一化。无须保存mini-batch的均值和方差，节省了存储空间。</p> <p>缺点：对于相似性相差较大的特征（比如颜色和大小），LN会降低模型的表示能力。</p> <h4 id="633-instance-normalization---实例规范化">6.3.3 Instance Normalization - 实例规范化</h4> <div class="row mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset=" D:\Work and study\Study\Nlp\组会20200413.assets\image-20200408113404640-480.webp 480w, D:\Work and study\Study\Nlp\组会20200413.assets\image-20200408113404640-800.webp 800w, D:\Work and study\Study\Nlp\组会20200413.assets\image-20200408113404640-1400.webp 1400w, " sizes="95vw" type="image/webp"></source> <img src="D:\Work%20and%20study\Study\Nlp\%E7%BB%84%E4%BC%9A20200413.assets\image-20200408113404640.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <p>](assets/img/old_mypic/image-20200408113404640.png)</p> <p>Instance norm和Batch norm的区别只有一点不同，那就是BN是作用于一个batch，而IN则是作用于单个样本。在一个channel内做归一化。</p> <h4 id="634-group-normalization---组规范化">6.3.4 Group Normalization - 组规范化</h4> <div class="row mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset=" D:\Work and study\Study\Nlp\组会20200413.assets\image-20200408113508859-480.webp 480w, D:\Work and study\Study\Nlp\组会20200413.assets\image-20200408113508859-800.webp 800w, D:\Work and study\Study\Nlp\组会20200413.assets\image-20200408113508859-1400.webp 1400w, " sizes="95vw" type="image/webp"></source> <img src="D:\Work%20and%20study\Study\Nlp\%E7%BB%84%E4%BC%9A20200413.assets\image-20200408113508859.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <p>](assets/img/old_mypic/image-20200408113508859.png)</p> <p>将channel方向分group，然后每个group内做归一化。<strong>LN和IN只是GN的两种极端形式。</strong></p> <h4 id="633-bn和ln的对比">6.3.3 BN和LN的对比</h4> <div class="row mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset=" D:\Work and study\Study\Nlp\组会20200413.assets\image-20200408112212220-480.webp 480w, D:\Work and study\Study\Nlp\组会20200413.assets\image-20200408112212220-800.webp 800w, D:\Work and study\Study\Nlp\组会20200413.assets\image-20200408112212220-1400.webp 1400w, " sizes="95vw" type="image/webp"></source> <img src="D:\Work%20and%20study\Study\Nlp\%E7%BB%84%E4%BC%9A20200413.assets\image-20200408112212220.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <p>](assets/img/old_mypic/image-20200408112212220.png)</p> <ul> <li>BN是“竖”着来的，各个维度分别做规范化，所以与batch size有关系；</li> <li>LN是“横”着来的，对于一个样本，不同的神经元neuron间做规范化；</li> </ul> <h3 id="64-normalization-效果">6.4 Normalization 效果</h3> <div class="row mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset=" D:\Work and study\Study\Nlp\组会20200413.assets\image-20200408114347725-480.webp 480w, D:\Work and study\Study\Nlp\组会20200413.assets\image-20200408114347725-800.webp 800w, D:\Work and study\Study\Nlp\组会20200413.assets\image-20200408114347725-1400.webp 1400w, " sizes="95vw" type="image/webp"></source> <img src="D:\Work%20and%20study\Study\Nlp\%E7%BB%84%E4%BC%9A20200413.assets\image-20200408114347725.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <p>](assets/img/old_mypic/image-20200408114347725.png)</p> <ol> <li> <p>权重伸缩不变性</p> <p><strong>权重伸缩不变性（weight scale invariance）</strong> 指的是：当权重W按照常量进行伸缩时，得到的规范化后的值保持不变。</p> </li> </ol> <div class="row mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset=" D:\Work and study\Study\Nlp\组会20200413.assets\image-20200408114516358-480.webp 480w, D:\Work and study\Study\Nlp\组会20200413.assets\image-20200408114516358-800.webp 800w, D:\Work and study\Study\Nlp\组会20200413.assets\image-20200408114516358-1400.webp 1400w, " sizes="95vw" type="image/webp"></source> <img src="D:\Work%20and%20study\Study\Nlp\%E7%BB%84%E4%BC%9A20200413.assets\image-20200408114516358.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <p>](assets/img/old_mypic/image-20200408114516358.png)</p> <div class="row mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset=" D:\Work and study\Study\Nlp\组会20200413.assets\image-20200408114525581-480.webp 480w, D:\Work and study\Study\Nlp\组会20200413.assets\image-20200408114525581-800.webp 800w, D:\Work and study\Study\Nlp\组会20200413.assets\image-20200408114525581-1400.webp 1400w, " sizes="95vw" type="image/webp"></source> <img src="D:\Work%20and%20study\Study\Nlp\%E7%BB%84%E4%BC%9A20200413.assets\image-20200408114525581.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <p>](assets/img/old_mypic/image-20200408114525581.png)</p> <p><strong>权重伸缩不变性可以有效地提高反向传播的效率。</strong></p> <div class="row mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset=" D:\Work and study\Study\Nlp\组会20200413.assets\image-20200408114550609-480.webp 480w, D:\Work and study\Study\Nlp\组会20200413.assets\image-20200408114550609-800.webp 800w, D:\Work and study\Study\Nlp\组会20200413.assets\image-20200408114550609-1400.webp 1400w, " sizes="95vw" type="image/webp"></source> <img src="D:\Work%20and%20study\Study\Nlp\%E7%BB%84%E4%BC%9A20200413.assets\image-20200408114550609.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <p>](assets/img/old_mypic/image-20200408114550609.png)</p> <p><strong>权重的伸缩变化不会影响反向梯度的 Jacobian 矩阵，因此也就对反向传播没有影响</strong>，避免了反向传播时因为权重过大或过小导致的梯度消失或梯度爆炸问题，从而加速了神经网络的训练。</p> <div class="row mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset=" D:\Work and study\Study\Nlp\组会20200413.assets\image-20200408114611299-480.webp 480w, D:\Work and study\Study\Nlp\组会20200413.assets\image-20200408114611299-800.webp 800w, D:\Work and study\Study\Nlp\组会20200413.assets\image-20200408114611299-1400.webp 1400w, " sizes="95vw" type="image/webp"></source> <img src="D:\Work%20and%20study\Study\Nlp\%E7%BB%84%E4%BC%9A20200413.assets\image-20200408114611299.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <p>](assets/img/old_mypic/image-20200408114611299.png)</p> <p>浅层的权重值越大，其梯度就越小。这样，参数的变化就越稳定，相当于实现了参数正则化的效果，避免参数的大幅震荡，提高网络的泛化性能。</p> <ol> <li><strong>Normalization 的数据伸缩不变性</strong></li> </ol> <div class="row mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset=" D:\Work and study\Study\Nlp\组会20200413.assets\image-20200408114822336-480.webp 480w, D:\Work and study\Study\Nlp\组会20200413.assets\image-20200408114822336-800.webp 800w, D:\Work and study\Study\Nlp\组会20200413.assets\image-20200408114822336-1400.webp 1400w, " sizes="95vw" type="image/webp"></source> <img src="D:\Work%20and%20study\Study\Nlp\%E7%BB%84%E4%BC%9A20200413.assets\image-20200408114822336.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <p>](assets/img/old_mypic/image-20200408114822336.png)</p> <p><strong>数据伸缩不变性仅对 BN、LN 和 IN和GN成立</strong>。因为这四者对输入数据进行规范化，因此当数据进行常量伸缩时，其均值和方差都会相应变化，分子分母互相抵消。而 WN 不具有这一性质。</p> <p><strong>数据伸缩不变性可以有效地减少梯度弥散，简化对学习率的选择。</strong></p> <div class="row mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset=" D:\Work and study\Study\Nlp\组会20200413.assets\image-20200408114911541-480.webp 480w, D:\Work and study\Study\Nlp\组会20200413.assets\image-20200408114911541-800.webp 800w, D:\Work and study\Study\Nlp\组会20200413.assets\image-20200408114911541-1400.webp 1400w, " sizes="95vw" type="image/webp"></source> <img src="D:\Work%20and%20study\Study\Nlp\%E7%BB%84%E4%BC%9A20200413.assets\image-20200408114911541.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <p>](assets/img/old_mypic/image-20200408114911541.png)</p> <p>每一层神经元的输出依赖于底下各层的计算结果。如果没有正则化，当下层输入发生伸缩变化时，经过层层传递，可能会导致数据发生剧烈的膨胀或者弥散，从而也导致了反向计算时的梯度爆炸或梯度弥散。</p> <p>加入 Normalization 之后，不论底层的数据如何变化，<strong>对于某一层神经元</strong>$h_{l}=f_{W_{l}}(x_{l})$而言，其输入$x_{l}$永远保持标准的分布，使得高层的训练更加简单。</p> <div class="row mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset=" https://s1.ax1x.com/2020/04/14/GzVRjf-480.webp 480w, https://s1.ax1x.com/2020/04/14/GzVRjf-800.webp 800w, https://s1.ax1x.com/2020/04/14/GzVRjf-1400.webp 1400w, " sizes="95vw" type="image/webp"></source> <img src="https://s1.ax1x.com/2020/04/14/GzVRjf.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <p>数据的伸缩变化也不会影响到对该层的权重参数更新，使得训练过程更加鲁棒，简化了对学习率的选择。</p> <h2 id="七自注意力机制">七、自注意力机制</h2> <h3 id="71-介绍与应用场景">7.1 介绍与应用场景</h3> <p><strong>自注意力</strong>，又称”intra-attention“，是一种在计算同一序列表示时，权重和序列的位置相关机制，被证明在机器阅读理解，抽象概要（abstractive summarization）和图片描述生成中非常有效。</p> <p>自注意力机制能够学习到当前词和句中先前词之前的关联性。</p> <div class="row mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset=" https://s1.ax1x.com/2020/04/14/GzVIEQ-480.webp 480w, https://s1.ax1x.com/2020/04/14/GzVIEQ-800.webp 800w, https://s1.ax1x.com/2020/04/14/GzVIEQ-1400.webp 1400w, " sizes="95vw" type="image/webp"></source> <img src="https://s1.ax1x.com/2020/04/14/GzVIEQ.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="row mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset=" https://s1.ax1x.com/2020/04/14/GzVoNj-480.webp 480w, https://s1.ax1x.com/2020/04/14/GzVoNj-800.webp 800w, https://s1.ax1x.com/2020/04/14/GzVoNj-1400.webp 1400w, " sizes="95vw" type="image/webp"></source> <img src="https://s1.ax1x.com/2020/04/14/GzVoNj.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <h3 id="72-自注意型网络和其他网络的不同">7.2 自注意型网络和其他网络的不同</h3> <p>局部编码：卷积神经网络显然是基于 N-gram 的局部编码；而对于循环神经网络，由于梯度消失等问题也只能建立短距离依赖。</p> <div class="row mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset=" https://s1.ax1x.com/2020/04/14/GzVT4s-480.webp 480w, https://s1.ax1x.com/2020/04/14/GzVT4s-800.webp 800w, https://s1.ax1x.com/2020/04/14/GzVT4s-1400.webp 1400w, " sizes="95vw" type="image/webp"></source> <img src="https://s1.ax1x.com/2020/04/14/GzVT4s.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <p>长距离依赖编码：</p> <div class="row mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset=" https://s1.ax1x.com/2020/04/14/GzVHCn-480.webp 480w, https://s1.ax1x.com/2020/04/14/GzVHCn-800.webp 800w, https://s1.ax1x.com/2020/04/14/GzVHCn-1400.webp 1400w, " sizes="95vw" type="image/webp"></source> <img src="https://s1.ax1x.com/2020/04/14/GzVHCn.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <p>实线表示为可学习的权重，虚线表示动态生成的权重。</p> <p>全连接网络虽然是一种非常直接的建模远距离依赖的模型， 但是无法处理变长的输入序列。不同的输入长度，其连接权重的大小也是不同的。可以利用注意力机制来“动态”地生成不同连接的权重，这就是<strong>自注意力模型（self-attention model）</strong>。由于自注意力模型的权重是<strong>动态生成</strong>的，因此可以处理变长的信息序列。</p> <h3 id="73-图解">7.3 图解</h3> <p>一对单词被输入到函数 f(⋅) 中，从而提取出它们之间的关系。对于某个特定的位置 t，有 T-1 对单词被归纳，而我们通过求和或平均或任意其它相关的技术对句子进行表征。当我们具体实现这个算法时，我们会对包括当前单词本身的 T 对单词进行这样的计算。</p> <div class="row mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset=" https://s1.ax1x.com/2020/04/14/GzVc9I-480.webp 480w, https://s1.ax1x.com/2020/04/14/GzVc9I-800.webp 800w, https://s1.ax1x.com/2020/04/14/GzVc9I-1400.webp 1400w, " sizes="95vw" type="image/webp"></source> <img src="https://s1.ax1x.com/2020/04/14/GzVc9I.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <p>α(⋅,⋅) 控制了每个单词组合可能产生的影响，和上式子的I(·,·)类似。在句子「I like you like this」中，两个单词「I」和「you」可能对于确定句子的情感没有帮助。然而，「I」和「like」的组合使我们对这句话的情感有了一个清晰的认识。在这种情况下，我们给予前一种组合的注意力很少，而给予后一种组合的注意力很多。通过引入权重向量 α(⋅,⋅)，我们可以让算法调整单词组合的重要程度。</p> \[h_{t} = \sum_{t'=1}^{T}\alpha(x_{t},x_{t'})f(x_{t},x_{t'})\] <div class="row mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset=" https://s1.ax1x.com/2020/04/14/GzVhDS-480.webp 480w, https://s1.ax1x.com/2020/04/14/GzVhDS-800.webp 800w, https://s1.ax1x.com/2020/04/14/GzVhDS-1400.webp 1400w, " sizes="95vw" type="image/webp"></source> <img src="https://s1.ax1x.com/2020/04/14/GzVhDS.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <p>如果我们把 10 个句子输入到网络中，我们会得到 10 个如下所示的注意力矩阵。</p> <div class="row mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset=" https://s1.ax1x.com/2020/04/14/GzV4Hg-480.webp 480w, https://s1.ax1x.com/2020/04/14/GzV4Hg-800.webp 800w, https://s1.ax1x.com/2020/04/14/GzV4Hg-1400.webp 1400w, " sizes="95vw" type="image/webp"></source> <img src="https://s1.ax1x.com/2020/04/14/GzV4Hg.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <h3 id="74-实现">7.4 实现</h3> <div class="row mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset=" https://s1.ax1x.com/2020/04/14/GzV2gP-480.webp 480w, https://s1.ax1x.com/2020/04/14/GzV2gP-800.webp 800w, https://s1.ax1x.com/2020/04/14/GzV2gP-1400.webp 1400w, " sizes="95vw" type="image/webp"></source> <img src="https://s1.ax1x.com/2020/04/14/GzV2gP.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <p>假设我们想要得到第 i 个单词的表征。对于包含第 i 个单词的单词组合，会生成两个输出：一个用于特征提取（绿色圆圈），另一个用于注意力加权（红色圆圈）。这两个输出可能共享同一个网络，但在本文中，我们为每个输出使用单独的网络。在得到最后的注意力权重之前，注意力（红色圆圈）的输出通过需要经过 sigmoid 和 softmax 层的运算。这些注意力权重会与提取出的特征相乘，以得到我们感兴趣的单词的表征。</p> <h3 id="75-参考来源">7.5 参考来源</h3> <p>[1] https://www.jianshu.com/p/9b922fb83d77</p> <p>[2] https://www.cnblogs.com/robert-dlut/p/8638283.html</p> <p>[3] https://www.infoq.cn/article/lteUOi30R4uEyy740Ht2</p> <h2 id="八多头注意力机制">八、多头注意力机制</h2> <div class="row mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset=" https://s1.ax1x.com/2020/04/14/GzVb3q-480.webp 480w, https://s1.ax1x.com/2020/04/14/GzVb3q-800.webp 800w, https://s1.ax1x.com/2020/04/14/GzVb3q-1400.webp 1400w, " sizes="95vw" type="image/webp"></source> <img src="https://s1.ax1x.com/2020/04/14/GzVb3q.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <p>多头attention（Multi-head attention）结构如上图，Query，Key，Value首先进过一个线性变换，然后输入到放缩点积attention，注意这里要做h次，也就是所谓的多头，每一次算一个头，<strong>头之间参数不共享，</strong>每次Q，K，V进行线性变换的参数</p> <div class="row mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset=" https://math.jianshu.com/math?formula=W-480.webp 480w, https://math.jianshu.com/math?formula=W-800.webp 800w, https://math.jianshu.com/math?formula=W-1400.webp 1400w, " sizes="95vw" type="image/webp"></source> <img src="https://math.jianshu.com/math?formula=W" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <p>是不一样的。然后将h次的放缩点积attention结果进行拼接，再进行一次线性变换得到的值作为多头attention的结果。</p> <p>可以看到，google提出来的多头attention的不同之处在于进行了h次计算而不仅仅算一次，论文中说到这样的<strong>好处</strong>是可以允许模型在不同的表示子空间里学习到相关的信息。</p> <div class="row mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset=" https://s1.ax1x.com/2020/04/14/GzVqg0-480.webp 480w, https://s1.ax1x.com/2020/04/14/GzVqg0-800.webp 800w, https://s1.ax1x.com/2020/04/14/GzVqg0-1400.webp 1400w, " sizes="95vw" type="image/webp"></source> <img src="https://s1.ax1x.com/2020/04/14/GzVqg0.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="row mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset=" https://s1.ax1x.com/2020/04/14/GxBXeU-480.webp 480w, https://s1.ax1x.com/2020/04/14/GxBXeU-800.webp 800w, https://s1.ax1x.com/2020/04/14/GxBXeU-1400.webp 1400w, " sizes="95vw" type="image/webp"></source> <img src="https://s1.ax1x.com/2020/04/14/GxBXeU.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <h2 id="九transformercnnrnn">九、Transformer/CNN/RNN</h2> <p>长距离依赖、位置信息、时间复杂度、串行并行</p> <h3 id="91-长距离依赖">9.1 长距离依赖</h3> <div class="row mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset=" https://s1.ax1x.com/2020/04/14/GzVLvV-480.webp 480w, https://s1.ax1x.com/2020/04/14/GzVLvV-800.webp 800w, https://s1.ax1x.com/2020/04/14/GzVLvV-1400.webp 1400w, " sizes="95vw" type="image/webp"></source> <img src="https://s1.ax1x.com/2020/04/14/GzVLvV.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <p>在特定的长距离特征捕获能力测试任务：Transformer&gt;RNN»CNN</p> <p>在比较远的距离上（主语谓语距离大于13）：Transformer ≈ RNN»CNN</p> <p>CNN解决这个问题是靠堆积深度来获得覆盖更长的输入长度的，所以CNN在这方面的表现与卷积核能够覆盖的输入距离最大长度有关系。如果通过增大卷积核的kernel size，同时加深网络深度，以此来增加输入的长度覆盖。</p> <p>Multi-head attention的head数量严重影响NLP任务中Long-range特征捕获能力：结论是head越多越有利于捕获long-range特征。</p> <h3 id="92-位置信息">9.2 位置信息</h3> <p>RNN：因为是线性序列结构，所以很自然它天然就会把位置信息编码进去。</p> <p>CNN：卷积核是能保留特征之间的相对位置的，道理很简单，滑动窗口从左到右滑动，捕获到的特征也是如此顺序排列，所以它在结构上已经记录了相对位置信息了。但是如果卷积层后面立即接上Pooling层的话，Max Pooling的操作逻辑是：从一个卷积核获得的特征向量里只选中并保留最强的那一个特征，所以到了<strong>Pooling层，位置信息就被扔掉了</strong>，这在NLP里其实是有信息损失的。所以在NLP领域里，目前CNN的一个发展趋势是抛弃Pooling层，靠全卷积层来叠加网络深度。</p> <div class="row mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset=" https://s1.ax1x.com/2020/04/14/GzVXuT-480.webp 480w, https://s1.ax1x.com/2020/04/14/GzVXuT-800.webp 800w, https://s1.ax1x.com/2020/04/14/GzVXuT-1400.webp 1400w, " sizes="95vw" type="image/webp"></source> <img src="https://s1.ax1x.com/2020/04/14/GzVXuT.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <p>Transformer：Self attention会让当前输入单词和句子中任意单词发生关系，然后集成到一个embedding向量里，但是当所有信息到了embedding后，位置信息并没有被编码进去。必须明确的在输入端将Positon信息编码，Transformer是用位置函数来进行位置编码的，而Bert等模型则给每个单词一个Position embedding，将单词embedding和单词对应的position embedding加起来形成单词的输入embedding。</p> <h3 id="93-串行并行能力">9.3 串行并行能力</h3> <p>RNN在并行计算方面有严重缺陷，这是它本身的序列依赖特性导致的。它的线形序列依赖性非常符合解决NLP任务。但是也正是这个线形序列依赖特性，导致它在并行计算方面要想获得质的飞跃，困难重重。</p> <p>CNN和Transformer来说，因为它们不存在网络中间状态不同时间步输入的依赖关系，所以可以非常方便及自由地做并行计算改造。</p> <p>Transformer ≈ CNN » RNN</p> <h3 id="94-计算复杂度">9.4 计算复杂度</h3> <div class="row mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset=" https://s1.ax1x.com/2020/04/14/GzVjDU-480.webp 480w, https://s1.ax1x.com/2020/04/14/GzVjDU-800.webp 800w, https://s1.ax1x.com/2020/04/14/GzVjDU-1400.webp 1400w, " sizes="95vw" type="image/webp"></source> <img src="https://s1.ax1x.com/2020/04/14/GzVjDU.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <p>三者单层的计算量：Transformer Block &gt; CNN &gt; RNN</p> <p>self attention：平方项是句子长度，因为每一个单词都需要和任意一个单词发生关系来计算attention，所以包含一个n的平方项。Transformer包含多层，其中的skip connection后的Add操作及LayerNorm操作不太耗费计算量，我先把它忽略掉，后面的FFN操作相对比较耗时，它的时间复杂度应该是n乘以d的平方。</p> <p>RNN：平方项是embedding size。</p> <p>CNN：平方项是embedding size。</p> <p>如果句子平均长度n大于embedding size，那么意味着Self attention的计算量要大于RNN和CNN；而如果反过来，就是说如果embedding size大于句子平均长度，那么明显RNN和CNN的计算量要大于self attention操作。一般正常的句子长度，平均起来也就几十个单词吧。而当前常用的embedding size从128到512。</p> <p>速度：self-attention &gt; RNN &gt; CNN &gt; Transformer</p> <h3 id="95-参考资料">9.5 参考资料</h3> <p>[1] <a href="https://zhuanlan.zhihu.com/p/54743941" rel="external nofollow noopener" target="_blank">https://zhuanlan.zhihu.com/p/54743941</a></p> <h2 id="十过拟合">十、过拟合</h2> <h3 id="101-过拟合的定义与宏观判断">10.1 过拟合的定义与宏观判断</h3> <p><strong>定义</strong>：给定一个假设空间H，一个假设h属于H，如果存在其他的假设h’属于H,使得在训练样例上h的错误率比h’小，但在整个实例分布上h’比h的错误率小，那么就说假设h过度拟合训练数据。</p> <div class="row mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset=" https://s1.ax1x.com/2020/04/14/GzVzE4-480.webp 480w, https://s1.ax1x.com/2020/04/14/GzVzE4-800.webp 800w, https://s1.ax1x.com/2020/04/14/GzVzE4-1400.webp 1400w, " sizes="95vw" type="image/webp"></source> <img src="https://s1.ax1x.com/2020/04/14/GzVzE4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="row mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset=" https://s1.ax1x.com/2020/04/14/GzVvbF-480.webp 480w, https://s1.ax1x.com/2020/04/14/GzVvbF-800.webp 800w, https://s1.ax1x.com/2020/04/14/GzVvbF-1400.webp 1400w, " sizes="95vw" type="image/webp"></source> <img src="https://s1.ax1x.com/2020/04/14/GzVvbF.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <h3 id="102-过拟合的原因">10.2 过拟合的原因</h3> <ol> <li> <p>训练集的数量级和模型的复杂度不匹配。训练集的数量级要小于模型的复杂度；</p> </li> <li> <p>训练集和测试集特征分布不一致；</p> </li> <li> <p>样本里的噪音数据干扰过大，大到模型过分记住了噪音特征，反而忽略了真实的输入输出间的关系；</p> </li> <li> <p>权值学习迭代次数足够多(Overtraining)，拟合了训练数据中的噪声和训练样例中没有代表性的特征。</p> </li> </ol> <h3 id="103-解决方案">10.3 解决方案</h3> <ol> <li> <p>调小模型复杂度：使模型适合自己训练集的数量级（缩小宽度和减小深度）</p> </li> <li> <p>增加数据：训练集越多，过拟合的概率越小</p> </li> <li> <p>正则化：参数太多，会导致我们的模型复杂度上升，容易过拟合，也就是我们的训练误差会很小。 正则化是指通过引入额外新信息来解决机器学习中过拟合问题的一种方法。这种额外信息通常的形式是模型复杂性带来的惩罚度。 正则化可以保持模型简单，另外，规则项的使用还可以约束我们的模型的特性。</p> </li> </ol> <div class="row mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset=" https://s1.ax1x.com/2020/04/14/GzZSUJ-480.webp 480w, https://s1.ax1x.com/2020/04/14/GzZSUJ-800.webp 800w, https://s1.ax1x.com/2020/04/14/GzZSUJ-1400.webp 1400w, " sizes="95vw" type="image/webp"></source> <img src="https://s1.ax1x.com/2020/04/14/GzZSUJ.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <p>a) L0范数：向量中非0的元素的个数。如果我们用L0范数来规则化一个参数矩阵W的话，就是希望W的大部分元素都是0即让参数W是稀疏的。</p> <p>b) L1范数：向量中各个元素绝对值之和，也叫“稀疏规则算子”（Lasso regularization）。</p> <table> <tbody> <tr> <td>c) L2范数：</td> <td> </td> <td>W</td> <td> </td> <td>^2。指向量各元素的平方和然后求平方根。我们让L2范数的规则项</td> <td> </td> <td>W</td> <td> </td> <td>^2最小，可以使得W的每个元素都很小，都接近于0，但与L1范数不同，它不会让它等于0，而是接近于0。</td> </tr> </tbody> </table> <ol> <li>dropout：在训练时候以一定的概率p来跳过一定的神经元。</li> </ol> <div class="row mt-3"> <figure> <picture> <source class="responsive-img-srcset" srcset=" https://s1.ax1x.com/2020/04/14/GzZp59-480.webp 480w, https://s1.ax1x.com/2020/04/14/GzZp59-800.webp 800w, https://s1.ax1x.com/2020/04/14/GzZp59-1400.webp 1400w, " sizes="95vw" type="image/webp"></source> <img src="https://s1.ax1x.com/2020/04/14/GzZp59.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <ol> <li> <p>early stopping：一种迭代次数截断的方法来防止过拟合的方法，即在模型对训练数据集迭代收敛之前停止迭代来防止过拟合。</p> <p>在每一个Epoch结束时（一个Epoch集为对所有的训练数据的一轮遍历）计算validation data的accuracy，当accuracy不再提高时，就停止训练。在训练的过程中，记录到目前为止最好的validation accuracy，当<strong>连续10次</strong>Epoch（或者更多次）没达到最佳accuracy时，则可以认为accuracy不再提高了。</p> </li> <li> <p>ensemble集成：集成学习算法也可以有效的减轻过拟合。Bagging通过平均多个模型的结果，来降低模型的方差。Boosting不仅能够减小偏差，还能减小方差。</p> </li> <li> <p>重新清晰数据：数据清洗从名字上也看的出就是把“脏”的“洗掉”，指发现并纠正数据文件中可识别的错误的最后一道程序，包括检查数据一致性，处理无效值和缺失值等。导致过拟合的一个原因也有可能是数据不纯导致的，如果出现了过拟合就需要我们重新清洗数据。</p> </li> </ol> <h2 id="十一attention-机制的大佬实现">十一、Attention 机制的大佬实现</h2> <p><a href="https://yq.aliyun.com/articles/342508?utm_content=m_39938" rel="external nofollow noopener" target="_blank">https://yq.aliyun.com/articles/342508?utm_content=m_39938</a></p> <p>来自中山大学数学学院的一个师兄写的文章，并附上了他的代码。个人对这些代码的理解还不够深入，后期请务必重新阅读。</p> <h2 id="十二拓展attention">十二、拓展Attention</h2> <p>参考文章： <a href="https://blog.csdn.net/qq_41058526/article/details/80578932" rel="external nofollow noopener" target="_blank">https://blog.csdn.net/qq_41058526/article/details/80578932</a></p> </div> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2024 Linchang Xiao. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?7b30caa5023af4af8408a472dc4e1ebb"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?9b43d6e67ddc7c0855b1478ee4c48c2d" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>